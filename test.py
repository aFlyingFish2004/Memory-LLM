import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from threading import Thread
import gradio as gr

# æ¨¡å‹è·¯å¾„
model_path = "models/Qwen2.5-1.5B-Read"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)
model.eval()

# ç³»ç»Ÿæç¤ºè¯
system_prompt = "You are a memory-augmented assistant. When you need information to answer a question, first generate a MEM_READ API call to retrieve relevant memory triplets. If the memory retrieval result is empty, respond truthfully by saying you don't know."

# æ„å»º prompt
def build_prompt(history, current_input):
    prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
    for turn in history:
        user = turn["content"] if turn["role"] == "user" else ""
        assistant = turn["content"] if turn["role"] == "assistant" else ""
        if user:
            prompt += f"<|im_start|>user\n{user}<|im_end|>\n"
        if assistant:
            prompt += f"<|im_start|>assistant\n{assistant}<|im_end|>\n"
    prompt += f"<|im_start|>user\n{current_input}<|im_end|>\n"
    prompt += "<|im_start|>assistant\n"
    return prompt

# æ¨ç†å‡½æ•°ï¼ˆè¿”å› OpenAI é£æ ¼å­—å…¸ï¼‰
def predict(user_input, history, top_p, temperature):
    history = history or []

    prompt = build_prompt(history, user_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    generation_kwargs = dict(
        **inputs,
        streamer=streamer,
        max_new_tokens=512,
        do_sample=True,
        top_p=top_p,
        temperature=temperature,
        repetition_penalty=1.1
    )

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    partial_output = ""
    new_history = history + [{"role": "user", "content": user_input}]
    for new_text in streamer:
        partial_output += new_text
        yield new_history + [{"role": "assistant", "content": partial_output}]

    # æœ€ç»ˆæ›´æ–°å®Œæ•´å†å²
    return new_history + [{"role": "assistant", "content": partial_output}]


# Gradioç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ’¬ æœ¬åœ°å¤§æ¨¡å‹å¯¹è¯ç•Œé¢ï¼ˆQwen2.5ï¼‰")

    chatbot = gr.Chatbot(type="messages")
    with gr.Row():
        with gr.Column(scale=8):
            user_input = gr.Textbox(placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", show_label=False)
        with gr.Column(scale=1):
            submit_btn = gr.Button("æäº¤")

    with gr.Accordion("å‚æ•°è®¾ç½®", open=False):
        top_p = gr.Slider(minimum=0.01, maximum=1.0, value=0.95, label="Top-p é‡‡æ ·å€¼")
        temperature = gr.Slider(minimum=0.01, maximum=1.5, value=0.95, label="æ¸©åº¦ç³»æ•°")

    clear_btn = gr.Button("æ¸…ç©ºå†å²")
    history_state = gr.State([])

    # æäº¤äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=predict,
        inputs=[user_input, history_state, top_p, temperature],
        outputs=[chatbot],
        show_progress=True
    ).then(
        lambda chat, _: (None, chat),
        inputs=[history_state, chatbot],
        outputs=[user_input, history_state]
    )

    user_input.submit(
        fn=predict,
        inputs=[user_input, history_state, top_p, temperature],
        outputs=[chatbot]
    ).then(
        lambda chat, _: (None, chat),
        inputs=[history_state, chatbot],
        outputs=[user_input, history_state]
    )

    clear_btn.click(lambda: ([], []), inputs=[], outputs=[chatbot, history_state])

# å¯åŠ¨ Gradio æœåŠ¡ï¼Œé¿å…ç«¯å£å†²çª
demo.launch(server_port=7861)
